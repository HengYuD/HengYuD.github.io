<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hadoop部署]]></title>
    <url>%2F2019%2F04%2F13%2FHadoop%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[环境Ubuntu 14.04.5 x64Hadoop 2.x.y Hadoop 有两个主要版本，Hadoop 1.x.y 和 Hadoop 2.x.y 系列，比较老的教材上用的可能是 0.20 这样的版本。Hadoop 2.x 版本在不断更新，本教程均可适用。如果需安装 0.20，1.2.1这样的版本，也可以作为参考，主要差别在于配置项 创建Hadoop用户(创建新用户是为了改环境的时候更方便一点)ctrl+alt+t打开终端 命令创建了可以登陆的 hadoop 用户，并使用 /bin/bash 作为 shellsudo useradd -m hadoop -s /bin/bash 设置密码sudo passwd hadoop 部署管理员权限sudo adduser hadoop sudo su hadoop切换用户 更新一下aptsudo apt-get update //sudo apt-get install vim 安装SSH、配置SSH无密码登陆集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server：sudo apt-get install openssh-server ssh localhost 首次登录提示输入yes看到hadoop@hadoop-Practise：即可确定登录成功 exit退出用ssh-keygen生成密钥 1234exit # 退出刚才的 ssh localhostcd ~/.ssh/ # 若没有该目录，请先执行一次ssh localhostssh-keygen -t rsa # 会有提示，都按回车就可以cat ./id_rsa.pub &gt;&gt; ./authorized_keys # 加入授权 在 Linux 系统中，~ 代表的是用户的主文件夹，即 “/home/用户名” 这个目录，如用户名为 hadoop，则 ~ 代表 “/home/hadoop/ 此时ssh localhost命令无需密码即可登录 安装Java环境sudo apt-get install openjdk-7-jre openjdk-7-jdkJava环境可选择 Oracle 的 JDK，或是 OpenJDK，OpenJDK 1.7 是没问题的。为方便，这边直接通过命令安装 OpenJDK 7（http://wiki.apache.org/hadoop/HadoopJavaVersions ） 安装好 OpenJDK 后，需要找到相应的安装路径，这个路径是用于配置JAVA_HOME 环境变量的。执行如下命令：dpkg -L openjdk-7-jdk | grep ‘/bin/javac’ 该命令会输出一个路径，除去路径末尾的 “/bin/javac”，剩下的就是正确的路径了。如输出路径为 /usr/lib/jvm/java-7-openjdk-amd64/bin/javac，则我们需要的路径为 /usr/lib/jvm/java-7-openjdk-amd64。 接着配置 JAVA_HOME 环境变量，为方便，我们在 ~/.bashrc 中进行设置vim ~/.bashrc source ~/.bashrc #使变量设置生效 echo $JAVA_HOME # 检验变量值 java -version $JAVA_HOME/bin/java -version # 与直接执行 java -version 一样 如果设置正确的话，$JAVA_HOME/bin/java -version 会输出 java 的版本信息，且和 java -version 的输出结果一样则Hadoop 所需的 Java 环境安装完成 安装 Hadoop 2（）在root用户的/usr/local/目录下下载hadoop-2.6.0.tar.gz文件wget https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz tar hadoop-2.6.0.tar.gz 解压安装包mv hadoop-2.6.0 hadoop 将安装目录由hadoop-2.6.0改为hadoop chown -R hadoop hadoop 将hadoop目录的拥有者设为hadoop用户-R : 对目前目录下的所有文件与子目录进行相同的拥有者变更(即以递回的方式逐个变更) 虽然Hadoop安装在/usr/local中，也是针对所有用户的，但是我们用账户hadoop来进行整个测试，所以更改文件权限不能不做，否则，在后面./sbin/start-dfs.sh步会遇到no such file or directory等错误 使用如下命令查看是否安装成功cd /usr/local/hadoop./bin/hadoop version 至此，hadoop安装成功 单机模式Hadoop本地模式只是用于本地开发调试，或者快速安装体验Hadoop 打开/usr/local/hadoop/etc/hadoop/hadoop-env.sh 将export JAVAHOME=${JAVAHOME}修改为export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64#视Java安装位置而定 (java的安装路径可用echo $JAVA_HOME查询) export HADOOP_HOME=/usr/local/hadoopexport PATH=$PATH:/usr/local/hadoop/bin 然后source hadoop-env.sh以保存修改此时单机模式就已配置完成。 单机模式测试采用Hadoop自带的示例WordCount来测试单机模式是否安装成功在/usr/local/hadoop路径下创input目录然后将README.txt文件拷贝到input目录中，然后执行 1bin/hadoop jar share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.7.3-sources.jar org.apache.hadoop.examples.WordCount input output 输出结果如下： 即可说明单机模式配置成功！ 打开output目录下的part 00000文件可以看到每个词的出现次数，实现了词频统计 伪分布式模式学习Hadoop一般是在伪分布式模式下进行。这种模式是在一台机器上各个进程上运行Hadoop的各个模块，伪分布式的意思是虽然各个模块是在各个进程上分开运行的，但是只是运行在一个操作系统上的，并不是真正的分布式。 沿用单机模式的配置，再修改core-site.xml和hafs-site.xml两个文件vim /usr/local/hadoop/etc/hadoop/core-site.xml原本文件中是如下内容 添加成如下内容： hadoop.tmp.dir file:/usr/local/hadoop/tmp Abase for other temporary directories. fs.defaultFS hdfs://localhost:9000 vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml添加成如下内容： dfs.replication 1 dfs.namenode.name.dir file:/usr/local/hadoop/tmp/dfs/name dfs.datanode.data.dir file:/usr/local/hadoop/tmp/dfs/data 注意，如果要变回单机模式，清空&lt;configuration&gt;标签中的内容即可。 且如果不是以YARN模式启动的话，无需配置mapred-site.xml。 格式化 namenode 在/usr/local/hadoop目录下以hadoop用户执行./bin/hdfs namenode -format 如上说明格式化namenode成功 ./sbin/start-dfs.sh 启动服务可以看到 用jps命令判断是否启动成功 如上，启动成功！ 若无Datanode和Namenode,说明配置不成功，针对这种情况，依次执行一下命令 1234./sbin/stop-dfs.sh # 关闭dfsrm -r ./tmp # 删除 tmp 文件，注意这会删除 HDFS 中原有的所有数据./bin/hdfs namenode -format # 重新格式化 NameNode./sbin/start-dfs.sh # 重启dfs 若 jps 命令’command not found’ ，由于jps是jdk自带的命令，找不到命令可能环境变量出了问题，用echo $PATH 查看环境变量，若无，则在hadoop用户的根目录下的./bashrc文件中配置JAVA的环境变量 export JAVA_HOME=/java的安装路径/export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH 然后source ./bashrc 保存应用即可 启动成功后可以打开http://localhost:50070管理页面查看namenode和datanode或者DFS信息（云服务器可以打开http://服务器地址:50070/） 伪分布模式测试单机模式直接读取的是本地文件，如input目录和output目录，而伪分布式模式需要读取分布式文件系统HDFS中的文件，所以还得先建立HDFS。 ./bin/hdfs dfs -mkdir -p /user/hadoop # 在HDFS中创建用户目录./bin/hdfs dfs -mkdir input # 在用户目录下创建input目录./bin/hdfs dfs -put ./etc/hadoop/*.xml input # 将一些测试文件移入input目录，就移配置xml文件作为输入 （相对路径input即可表示/user/hadoop/input） 然后执行一个筛出dfs开头的单词的程序： 其中./bin/hadoop jar向集群提交作业 grep是Hadoop提供的一个示例程序 ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output ‘dfs[a-z.]+’ 查看输出，此时输出仍在HDFS中 可以将HDFS中的文件拷到本地./bin/hdfs dfs -get output ./output 注意第二次运行的时候需要删除HDFS中的output，否则会报错，因为此中有防止覆盖的机制，不会在你在不知情的情况下就覆盖掉上一个output。 最后所有程序运行结束后运行./sbin/stop-dfs.sh停止守护进程即可 wordcount测试 先删除HDFS中的output目录：./bin/hdfs dfs -rm -r output 然后如同单机模式中运行WordCount程序一样，运行：bin/hadoop jar share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.6.0-sources.jar org.apache.hadoop.examples.WordCount input output 完全分布模式完全分布式模式才是生产环境采用的模式，Hadoop运行在服务器集群上，生产环境一般都会做HA，以实现高可用 HA安装HA是指高可用，为了解决Hadoop单点故障问题，生产环境一般都做HA部署。]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F04%2F11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
